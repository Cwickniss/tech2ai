{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e06c733a-1124-44b5-a634-37d0887fdfe6",
   "metadata": {},
   "source": [
    "# If you're interested - this is a variation of Lab2 with document pre-processing\n",
    "\n",
    "## RAG (Retrieval Augmented Generation\n",
    "\n",
    "For our 2nd agent, we will be asking GPT-4o-mini to estimate the price of one of our deals.\n",
    "\n",
    "It turns out that LLMs are really good at this! Out of the box, GPT-4o achieves an average error of $76, much better than our Neural Network and traditional solutions.\n",
    "\n",
    "But we can do even better: we'll provide it with some context, in the form of 5 similar products from our training dataset\n",
    "\n",
    "Again I'll be going quite quickly through this - the idea is for you to run this yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2db71ba5-55a8-48b7-97d5-9db8dc872837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "import chromadb\n",
    "from items import Item\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "from testing import Tester\n",
    "from openai import OpenAI\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b044d040-e467-4463-a3a5-119939ca8199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN', 'your-key-if-not-using-env')\n",
    "DB = \"preprocessed_vectorstore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c1cb7f1-41f7-4df8-95fa-f3143b4ce312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Log in to HuggingFace\n",
    "# If you don't have a HuggingFace account, you can set one up for free at www.huggingface.co\n",
    "# And then add the HF_TOKEN to your .env file as explained in the project README\n",
    "\n",
    "hf_token = os.environ['HF_TOKEN']\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6b8a0f3-af5c-4f21-8a5f-a4df4fa420ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Delphi FG0166 Fuel Pump Module = $226.95>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training data\n",
    "\n",
    "with open('../train.pkl', 'rb') as file:\n",
    "    train = pickle.load(file)\n",
    "\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf43f181-8b51-43c8-9763-599220cf6e66",
   "metadata": {},
   "source": [
    "# Now create a Chroma Datastore\n",
    "\n",
    "Now we will use the free, open-source Vector database Chroma.  \n",
    "We will create a Chroma datastore with 400,000 products from our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba77914-ea9a-4b92-9280-863ee07ca8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=DB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1744c683-847a-4151-b6e0-56066f1fe4b0",
   "metadata": {},
   "source": [
    "# Introducing the SentenceTransfomer Encoding LLM\n",
    "\n",
    "The all-MiniLM is a very useful model from HuggingFace that maps sentences & paragraphs to 384 dimensional vectors and is ideal for tasks like semantic search.\n",
    "\n",
    "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "It can run pretty quickly locally.\n",
    "\n",
    "As an alternative, OpenAI provides a closed-source Embeddings model. Benefits compared to OpenAI embeddings:\n",
    "1. It's free and fast!\n",
    "3. We can run it locally, so the data never leaves our box - might be useful if you're building a personal RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af2545a0-e160-41db-8914-f77b1c7eff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32abb023-64b5-40a4-bfc1-e22c3ec31221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in a list of texts, get back a numpy array of vectors\n",
    "\n",
    "vector = model.encode([\"A room full of software engineers\"])[0]\n",
    "print(vector.shape)\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7ec5b4-da3d-45d4-ab70-865a48fbb8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "vector1 = model.encode([\"A room full of software engineers\"])[0]\n",
    "vector2 = model.encode([\"A room full of AI Data Scientists\"])[0]\n",
    "cosine_similarity(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51798a0f-0f8b-45dd-a580-c6b149ef6eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1 = model.encode([\"A room full of software engineers\"])[0]\n",
    "vector3 = model.encode([\"Curried eggplant\"])[0]\n",
    "cosine_similarity(vector1, vector3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66fe9b9-b831-4595-84b1-7d128d9d02c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sentence equivalent of the famous King - Man + Woman = Queen\n",
    "\n",
    "room_engineers = model.encode([\"A room full of software engineers\"])[0]\n",
    "room_scientists = model.encode([\"A room full of AI Data Scientists\"])[0]\n",
    "engineers = model.encode([\"software engineers\"])[0]\n",
    "scientists = model.encode([\"AI Data Scientists\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66848aad-7a34-4034-b887-94cc356fdbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(room_engineers, room_scientists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1290cc25-7dbb-4b76-bbcf-14b2e7e29988",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_room = room_engineers - engineers + scientists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72865ef1-5cd8-49b4-8307-81055f4e130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(new_room, room_scientists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6585129c-1ff4-4acc-af22-20975954678d",
   "metadata": {},
   "source": [
    "## With that background, let's populate our Chroma database\n",
    "\n",
    "### By calculating vectors for 400,000 scraped products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dce18a7-d62e-425d-96fb-85ed850e2323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "groq = Groq()\n",
    "preprocess_model = \"llama-3.1-8b-instant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d8e2a13b-ec7a-4210-970e-9475f5e3fa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(item):\n",
    "    user_message = \"Write a 2-3 sentence summary of this product; include all facts that affect its price:\\n\"\n",
    "    user_message += item\n",
    "    user_message += \"\\n\\nReply only with the summary, no introduction\"\n",
    "    messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "    response = groq.chat.completions.create(\n",
    "        model=preprocess_model,\n",
    "        messages=messages,\n",
    "        seed=42,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4071afe7-439c-40de-bd47-b0e66dde48fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Power Stop Rear Z36 Truck and Tow Brake Kit with Calipers\\nThe Power Stop Z36 Truck & Tow Performance brake kit provides the superior stopping power demanded by those who tow boats, haul loads, tackle mountains, lift trucks, and play in the harshest conditions. The brake rotors are drilled to keep temperatures down during extreme braking and slotted to sweep away any debris for constant pad contact. Combined with our Z36 Carbon-Fiber Ceramic performance friction formulation, you can confidently push your rig to the limit and look good doing it with red powder brake calipers. Components are engineered to handle the stress of towing, hauling, mountainous driving, and lifted trucks. Dust-free braking performance. Z36 Carbon-Fiber Ceramic formula provides the extreme braking performance demanded by your truck or 4x'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1fd2cac8-3ce6-4244-8165-12b3fada53ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Power Stop Rear Z36 Truck and Tow Brake Kit with Calipers provides superior stopping power for heavy-duty trucks and 4x4 vehicles, featuring drilled and slotted brake rotors for cooling and debris removal, combined with a Z36 Carbon-Fiber Ceramic performance formula and red powder brake calipers. This kit is designed to handle the stress of towing, hauling, mountainous driving, and lifted trucks. The components are built for extreme braking performance, resulting in dust-free braking.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(train[1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b1c8f101-9c81-462d-be2e-9b479831857f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▋                                                                                                                                                          | 18/4000 [02:19<8:34:18,  7.75s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train), \u001b[32m100\u001b[39m)):\n\u001b[32m     15\u001b[39m     documents = [item.text \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m train[i: i+\u001b[32m100\u001b[39m]]\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     preprocessed = \u001b[38;5;28mlist\u001b[39m(executor.map(preprocess, documents))\n\u001b[32m     17\u001b[39m     vectors = model.encode(preprocessed).astype(\u001b[38;5;28mfloat\u001b[39m).tolist()\n\u001b[32m     18\u001b[39m     metadatas = [{\u001b[33m\"\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m\"\u001b[39m: item.category, \u001b[33m\"\u001b[39m\u001b[33mprice\u001b[39m\u001b[33m\"\u001b[39m: item.price} \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m train[i: i+\u001b[32m100\u001b[39m]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tech2ai/lib/python3.11/concurrent/futures/_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tech2ai/lib/python3.11/concurrent/futures/_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tech2ai/lib/python3.11/concurrent/futures/_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tech2ai/lib/python3.11/threading.py:327\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Check if the collection exists; if not, create it\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "  \n",
    "    collection_name = \"products\"\n",
    "    existing_collection_names = client.list_collections()\n",
    "    reset = True\n",
    "    \n",
    "    if reset or (collection_name not in existing_collection_names):\n",
    "        client.delete_collection(collection_name)\n",
    "        collection = client.create_collection(collection_name)\n",
    "        for i in tqdm(range(0, len(train), 100)):\n",
    "            documents = [item.text for item in train[i: i+100]]\n",
    "            preprocessed = list(executor.map(preprocess, documents))\n",
    "            vectors = model.encode(preprocessed).astype(float).tolist()\n",
    "            metadatas = [{\"category\": item.category, \"price\": item.price} for item in train[i: i+100]]\n",
    "            ids = [f\"doc_{j}\" for j in range(i, i+100)]\n",
    "            collection.add(\n",
    "                ids=ids,\n",
    "                documents=documents,\n",
    "                embeddings=vectors,\n",
    "                metadatas=metadatas\n",
    "            )\n",
    "    collection = client.get_or_create_collection(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f80a6ff-e620-4b18-97f6-14355e8b350a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c65375e1-a8eb-4203-b8f1-dfff69a693cc",
   "metadata": {},
   "source": [
    "# Let's visualize the vectorized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202c3b08-dc89-4995-a25c-041417ec9b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is very fun turning this up to 400_000 and seeing the full dataset visualized,\n",
    "# but it almost crashes my box every time so do that at your own risk!! 10_000 is safe!\n",
    "\n",
    "MAXIMUM_DATAPOINTS = 5_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c653565a-6405-4c5a-b925-7e14a17bf2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = ['Appliances', 'Automotive', 'Cell_Phones_and_Accessories', 'Electronics','Musical_Instruments', 'Office_Products', 'Tools_and_Home_Improvement', 'Toys_and_Games']\n",
    "COLORS = ['cyan', 'blue', 'brown', 'orange', 'yellow', 'green' , 'purple', 'red']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a754334-69ef-4b4f-92c7-d7da89457f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prework\n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'], limit=MAXIMUM_DATAPOINTS)\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "categories = [metadata['category'] for metadata in result['metadatas']]\n",
    "colors = [COLORS[CATEGORIES.index(c)] for c in categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a30b5e9-7dd9-45c1-a9a7-74cb22cdef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a 2D chart\n",
    "# TSNE stands for t-distributed Stochastic Neighbor Embedding - it's a common technique for reducing dimensionality of data\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, n_jobs=-1)\n",
    "reduced_vectors = tsne.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a97fc5-9f44-4f1d-a253-8c8f0bcd9ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=3, color=colors, opacity=0.7),\n",
    "    text=[f\"Category: {c}<br>Text: {d[:50]}...\" for c, d in zip(categories, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='2D Chroma Vectorstore Visualization',\n",
    "    scene=dict(xaxis_title='x', yaxis_title='y'),\n",
    "    width=1200,\n",
    "    height=800,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdb35e3-bd0d-4569-872b-34bea8316675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 3D!\n",
    "\n",
    "tsne = TSNE(n_components=3, random_state=42, n_jobs=-1)\n",
    "reduced_vectors = tsne.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4361c151-9f1b-4652-9204-695baf3860d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    z=reduced_vectors[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=3, color=colors, opacity=0.7),\n",
    "    text=[f\"Category: {c}<br>Text: {d[:50]}...\" for c, d in zip(categories, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
    "    width=1200,\n",
    "    height=800,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f14728-b797-49ed-9aad-e98ea6946b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now - set up OpenAI, Ollama and DeepSeek\n",
    "\n",
    "# OpenAI\n",
    "openai = OpenAI()\n",
    "\n",
    "# Ollama\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "# DeepSeek\n",
    "deepseek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "deepseek_via_openai_client = OpenAI(api_key=deepseek_api_key,base_url=\"https://api.deepseek.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a270b0ad-5a8f-4e54-a852-f16992314e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the test pickle file\n",
    "\n",
    "with open('../test.pkl', 'rb') as file:\n",
    "    test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74a25b1-f93c-4a75-9999-09e262f9abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to give some context to GPT-4o-mini by selecting 5 products with similar descriptions\n",
    "\n",
    "def make_context(similars, prices):\n",
    "    message = \"To provide some context, here are some other items that might be similar to the item you need to estimate.\\n\\n\"\n",
    "    for similar, price in zip(similars, prices):\n",
    "        message += f\"Potentially related product:\\n{similar}\\nPrice is ${price:.2f}\\n\\n\"\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b57490-060d-47ff-9cf0-2b61b455bcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(item, similars, prices):\n",
    "    system_message = \"You estimate prices of items. Reply only with the price, no explanation\"\n",
    "    user_prompt = make_context(similars, prices)\n",
    "    user_prompt += \"And now the question for you:\\n\\n\"\n",
    "    user_prompt += item.test_prompt().replace(\" to the nearest dollar\",\"\").replace(\"\\n\\nPrice is $\",\"\")\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": \"Price is $\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ea237-5cae-4b2d-ab13-0e63bef3b4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2\n",
    "!ollama pull deepseek-r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59a8ba0-acfb-4cce-9105-2d8a9d9bd936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(item):\n",
    "    system_message = \"You rewrite product descriptions in a format most suitable for finding similar products in a Knowledge Base\"\n",
    "    user_message = \"Please write a short 2-3 sentence description of the following product; your description will be used to find similar products so it should be comprehensive and only about the product. Details:\\n\"\n",
    "    user_message += item\n",
    "    user_message += \"\\n\\nNow please reply only with the short description, with no introduction\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": user_message}]\n",
    "    response = ollama_via_openai.chat.completions.create(\n",
    "        model=\"llama3.2\",\n",
    "        messages=messages,\n",
    "        seed=42\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c8dc15-2fa0-4b5f-b9f0-6f422c2eb60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(\"A Shure MV7+ professional condenser mic for podcasting with exceptional audio quality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a99ce28-7e42-45df-9aa3-e115332f9fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector(item):\n",
    "    text = preprocess(item.text)\n",
    "    return model.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d471584f-4998-469f-8c7f-cd7ffc74b2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similars(item):\n",
    "    vec = vector(item)\n",
    "    results = collection.query(query_embeddings=vec.astype(float).tolist(), n_results=5)\n",
    "    documents = results['documents'][0][:]\n",
    "    prices = [m['price'] for m in results['metadatas'][0][:]]\n",
    "    return documents, prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37742d64-0007-4259-8175-212ebb50fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test[1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0de8ad7-cc42-4a37-9539-ea1f65af1165",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocess(test[1].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de25d64b-655c-4680-8a2a-b4158abf45ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, prices = find_similars(test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5070e315-dbea-4dc2-8891-395848e7dba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(make_context(documents, prices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0700ba02-a84b-432b-896a-29de50b85569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function that extracts a price from a response from GPT-4o-mini\n",
    "\n",
    "def get_price(s):\n",
    "    s = s.replace('$','').replace(',','')\n",
    "    match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", s)\n",
    "    return float(match.group()) if match else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce59e40-391b-4e52-b190-ef917b2baaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_price(\"blah blah the price is $99.99 so cheap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721ab130-b6d8-4356-9704-687c9bc2636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function for gpt-4o-mini\n",
    "\n",
    "def gpt_4o_mini_rag(item):\n",
    "    \n",
    "    # RAG - lookup similar items from our KnowledgeBase\n",
    "    documents, prices = find_similars(item)\n",
    "\n",
    "    # RAG - enrich the prompt to include the similar items\n",
    "    messages = messages_for(item, documents, prices)\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", \n",
    "        messages=messages,\n",
    "        seed=42,\n",
    "        max_tokens=5\n",
    "    )\n",
    "    reply = response.choices[0].message.content\n",
    "    return get_price(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613a6278-2da5-4bf8-b733-2947736feb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the actual price of this per Amazon?\n",
    "\n",
    "test[1].price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30586f2-6b84-4750-acf5-a113ac9ccb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, time for gpt-4o-mini plus RAG to try:\n",
    "\n",
    "gpt_4o_mini_rag(test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3ead73-09e2-4d2d-8239-81c38cf7dc3e",
   "metadata": {},
   "source": [
    "# Were you following that?\n",
    "\n",
    "Let's do it again with some print statements.\n",
    "\n",
    "This is a \"DIY\" version of RAG; we're not using an abstraction layer like langchain to build the prompt, we're simply doing it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e90fdcd-5db9-4b58-9718-2c08cb76221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function for gpt-4o-mini, now with print statements\n",
    "\n",
    "def gpt_4o_mini_rag_explainer(item):\n",
    "    documents, prices = find_similars(item)\n",
    "    print(f\"Asking GPT-4o-mini to estimate the price of {item.title}\")\n",
    "    print(f\"Given similar prices of these items:\")\n",
    "    for document, price in zip(documents, prices):\n",
    "        similar = document.split(\"\\n\")[0]\n",
    "        print(f\"Similar item: {similar} costs ${price:.2f}\")\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", \n",
    "        messages=messages_for(item, documents, prices),\n",
    "        seed=42,\n",
    "        max_tokens=5\n",
    "    )\n",
    "    reply = response.choices[0].message.content\n",
    "    print(f\"\\n\\nGPT-4o-mini reponded: {reply}\")\n",
    "    price = get_price(reply)\n",
    "    print(f\"Extracted price is {price:.2f}\")\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f7fff-40e1-4336-b634-f19dfa7c07a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_mini_rag_explainer(test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db52ce7c-0fd5-4f43-b429-f8bc45a953c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the 7B variant of DeepSeek-R1 (the distilled version based on Qwen)\n",
    "# A bit unpredictable and can take a long time!\n",
    "# I needed to make the prompt a little more explicit otherwise it refused to answer sometimes..\n",
    "\n",
    "def deepseek_local_rag(item):\n",
    "    documents, prices = find_similars(item)\n",
    "    messages = messages_for(item, documents, prices)\n",
    "    messages[1][\"content\"] += \"\\nYou only need to guess the price, using the similar items to give you some reference point. Reply only with the price. Only think briefly; avoid overthinking.\"\n",
    "    response = ollama_via_openai.chat.completions.create(\n",
    "        model=\"deepseek-r1\", \n",
    "        messages=messages,\n",
    "        seed=42,\n",
    "    )\n",
    "    reply = response.choices[0].message.content\n",
    "    print(reply)\n",
    "    if \"</think>\" in reply:\n",
    "        reply = reply.split(\"</think>\")[1]\n",
    "    return get_price(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a19a89-5648-4e43-8bbc-0b2db1e76413",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek_local_rag(test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606c4272-948a-4cbb-b0d2-8efc242066d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the full DeepSeek-V3 \n",
    "\n",
    "def deepseek_api_rag(item):\n",
    "    documents, prices = find_similars(item)\n",
    "    response = deepseek_via_openai_client.chat.completions.create(\n",
    "        model=\"deepseek-chat\", \n",
    "        messages=messages_for(item, documents, prices),\n",
    "        seed=42,\n",
    "        max_tokens=8\n",
    "    )\n",
    "    reply = response.choices[0].message.content\n",
    "    return get_price(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdbfbee-f138-4b9c-808f-5150323fbc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek_api_rag(test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdaa62b-00a2-4998-9949-e0b4b21404fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the distilled version of DeepSeek-R1 with 70B parameters via Groq\n",
    "\n",
    "from groq import Groq\n",
    "model_name = \"deepseek-r1-distill-llama-70b\"\n",
    "groq = Groq()\n",
    "\n",
    "def deepseek_distilled_rag(item):\n",
    "    documents, prices = find_similars(item)\n",
    "    messages = messages_for(item, documents, prices)\n",
    "    messages[1][\"content\"] += \"\\nYou only need to guess the price, using the similar items to give you some reference point. Reply only with the price. Only think briefly; avoid overthinking.\"\n",
    "    response = groq.chat.completions.create(\n",
    "        model=model_name, \n",
    "        messages=messages,\n",
    "        seed=42,\n",
    "        max_tokens=8\n",
    "    )\n",
    "    reply = response.choices[0].message.content\n",
    "    return get_price(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc5cc32-cd9f-4a9d-ab10-70137c8d8288",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek_distilled_rag(test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff87932b-71c3-4269-8be6-aac420617b39",
   "metadata": {},
   "source": [
    "## We will kick off the next line then take a 5 minute break\n",
    "\n",
    "## When we come back: unveiling a proprietary fine-tuned LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91d83ca-6a8e-4d8f-9bac-eaf40674a4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tester.test(deepseek_distilled_rag, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469ece37-b029-4539-ba70-18985758a102",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "root.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c826a3f2-33a3-4c70-a53d-2a78c1776304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.frontier_agent import FrontierAgent\n",
    "\n",
    "frontier = FrontierAgent(collection)\n",
    "frontier.price(\"Quadcast HyperX condenser mic, connects via usb-c to your computer for crystal clear audio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88645370-c74d-48c1-ac88-5735b0f93f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
