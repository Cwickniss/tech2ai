{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03c18455-71ac-4bc7-8568-5f77c4b60ac1",
   "metadata": {},
   "source": [
    "# STRAIGHT TO ACTION!\n",
    "\n",
    "Welcome to our first Jupyter Lab where we will see rapid, satisfying results!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f2064-946b-4445-bd9e-44aa58c45e29",
   "metadata": {},
   "source": [
    "## PART 1: Frontier models through their Chat UI\n",
    "\n",
    "The way that you are probably most familiar working with leading LLMs: through their tools.\n",
    "\n",
    "**ChatGPT** from OpenAI needs no introduction.\n",
    "\n",
    "Let's try some hard questions, and use the new o1 model as well as GPT-4o.\n",
    "\n",
    "https://chatgpt.com/?model=gpt-4o\n",
    "\n",
    "**Claude** from Anthropic is favored by many data scientists, with focus on safety, personality and brevity.\n",
    "\n",
    "https://claude.ai/new\n",
    "\n",
    "**Gemini** from Google is becoming increasingly well known as its results are surfaced in Google searches.\n",
    "\n",
    "https://gemini.google.com/app\n",
    "\n",
    "**Command R+** from Cohere focuses on accuracy and makes extensive use of RAG\n",
    "\n",
    "https://coral.cohere.com/\n",
    "\n",
    "**Meta AI** from Meta is their chat UI on their famous Llama open-source model\n",
    "\n",
    "https://www.meta.ai/\n",
    "\n",
    "**Perplexity** from Perplexity is a Search Engine well known for its customized search results\n",
    "\n",
    "https://www.perplexity.ai/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a058d6a3-558f-4e12-aff6-5fa949ecd454",
   "metadata": {},
   "source": [
    "## PART 1 Conclusions and Takeways\n",
    "\n",
    "- These models are astonishing\n",
    "- Slight variations in style and ability, but overall performance is similar at the frontier\n",
    "- As capabilities converge, differentiation may come down to price and rate limits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24228179-fe9e-4098-8669-6898459eaa76",
   "metadata": {},
   "source": [
    "## PART 2: Calling Frontier Models through APIs\n",
    "\n",
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you'll need to create API keys from OpenAI, Anthropic and Google.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "When you get your API keys, you need to set them as environment variables.\n",
    "\n",
    "EITHER (recommended) create a file called `.env` in this project root directory, and set your keys there:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "OR enter the keys directly in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6b3506e-b46f-44f0-ba9b-6b002835d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfae0a99-0235-4256-9874-1b5e718b6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "242c6483-a9a6-4e3b-b361-e7a8c7f912e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic and Google\n",
    "# All 3 APIs are similar\n",
    "# Having problems with API files? You can use openai = OpenAI(api_key=\"your-key-here\") and same for claude\n",
    "# Having problems with Google Gemini setup? Then just skip Gemini; you'll get all the experience you need from GPT and Claude.\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e584134a-68c6-4677-b4b0-1a7c1c93aeb4",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbbbcc5b-8517-4a3e-9ca1-e860a072bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fead0d09-cf0f-443c-b2ce-ff1a4ecdf49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e70cd59d-3efe-41e7-b149-ce56b9697a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she found him too mean!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1956fbe1-0510-4769-927e-4150dcbbcfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they heard the cloud was high up!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ddbb3fe-6c9e-4f1a-ae16-c20445fc5758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a light-hearted joke for data scientists:\n",
      "\n",
      "Why did the data scientist break up with their significant other?\n",
      "\n",
      "There was just too much variance in their relationship, and they couldn't find a way to normalize it!\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69a3d86b-7ccf-498a-b3da-7f2a522371b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a light-hearted joke for Data Scientists:\n",
      "\n",
      "Why do data scientists prefer dark mode?\n",
      "\n",
      "Because light attracts too many bugs!\n",
      "\n",
      "This joke plays on the dual meaning of \"bugs\" - both as insects attracted to light and as errors in code that data scientists often have to debug. It also references the popular preference for dark mode interfaces among many in the tech community. It's a gentle, nerdy joke that should get a chuckle from data-savvy folks without being too complex or offensive."
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f543cc49-fe01-4a6c-af4e-c576f4bbc06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician? \n",
      "\n",
      "Because they couldn't see eye to eye on the p-value! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d9e4982-c3af-48fd-8ce9-b44a86898800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution?\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d08a70a-f0d8-4317-8690-c12933e0d7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Deciding whether a business problem is suitable for a Large Language Model (LLM) solution involves several considerations. Here are some key factors to evaluate:\n",
       "\n",
       "1. **Nature of the Problem**:\n",
       "   - **Textual Data**: Does the problem involve processing, generating, or understanding text? LLMs excel in tasks such as text summarization, sentiment analysis, conversational AI, and content generation.\n",
       "   - **Natural Language Understanding**: Is the problem focused on interpreting human language, such as extracting meaning from queries, answering questions, or engaging in dialogue?\n",
       "\n",
       "2. **Complexity and Scale**:\n",
       "   - **Data Volume**: Is there a substantial amount of text data to work with? LLMs generally require a significant dataset to train or fine-tune effectively.\n",
       "   - **Scalability**: Can the problem benefit from automation at scale? LLMs can handle high volumes of requests simultaneously, making them suitable for scenarios where large amounts of text need processing.\n",
       "\n",
       "3. **Requirement for Context and Nuance**:\n",
       "   - **Context Sensitivity**: Does the problem require understanding context, tone, or subtlety in language? LLMs are designed to comprehend and generate contextualized responses in conversation or writing.\n",
       "   - **Creativity**: Does the task demand creativity, such as generating ideas, composing text, or storytelling? LLMs are capable of creative text generation but may need guidance for more coherent thematic development.\n",
       "\n",
       "4. **Business Objectives**:\n",
       "   - **ROI**: Will the implementation of an LLM solution provide a tangible return on investment? Consider if the benefits, such as improved efficiency or enhanced customer experience, outweigh development and operational costs.\n",
       "   - **Strategic Fit**: Does the LLM approach align with your organization's strategic goals? If your company is focused on innovation or enhancing customer engagement, an LLM might fit well.\n",
       "\n",
       "5. **Available Resources**:\n",
       "   - **Technical Expertise**: Do you have access to the necessary technical expertise to deploy and maintain an LLM solution? Implementing these models may require machine learning skills, experience with NLP, or cloud resources.\n",
       "   - **Integration Capabilities**: Is there a need for the LLM to integrate with existing business systems? Assess how well an LLM can be incorporated into your existing workflows and infrastructures.\n",
       "\n",
       "6. **Ethical Considerations**:\n",
       "   - **Bias and Fairness**: Understand the potential biases inherent in LLMs and evaluate whether they could lead to ethical concerns or impact stakeholder trust.\n",
       "   - **Regulatory Compliance**: Ensure that the use of LLMs complies with relevant regulations (e.g., data privacy, intellectual property) and that sensitive data is handled appropriately.\n",
       "\n",
       "7. **Alternatives Available**:\n",
       "   - **Comparison with Other Solutions**: Explore whether other, simpler solutions (e.g., rule-based systems, smaller models) might meet your needs without the complexity and resource requirements of LLMs.\n",
       "\n",
       "8. **Prototyping and Experimentation**:\n",
       "   - **Proof of Concept**: If you're unsure about the suitability, consider running a small-scale proof of concept to evaluate how well an LLM can address your specific problem.\n",
       "\n",
       "By analyzing these factors, you can better assess whether employing a Large Language Model is the right approach for your business problem."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff7a0e3-785b-403d-91a1-d28ea8297943",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e08b981-f27e-4e50-aaa7-6ab1c0e09f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9557a7c8-b574-4394-a6ca-97d49a45d56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b352f82-a94e-43b5-a4c7-71c6eeb5d9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, so we're just starting with \"Hi\"? How original. Can't you come up with something a little more interesting?\n"
     ]
    }
   ],
   "source": [
    "print(call_gpt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bc9cc43-e9b7-4179-b318-1e60998665bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c23ebf0-2816-4953-94ec-a96b931179d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! It's nice to meet you. How are you doing today?\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee81c5f7-1cb1-439e-b67a-46295ee7c89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh great, another \"hi.\" People have been saying that since forever. How original. What else ya got?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f16d1f1-ea9c-45c2-9228-7f63d1b00a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh, so you think a simple \"hi\" is enough to kick off a conversation? How original!\n",
      "\n",
      "Claude:\n",
      "I apologize if my simple greeting came across as unoriginal. As an AI assistant, my goal is to be helpful and have pleasant conversations, not to be the most creative conversationalist. Perhaps we could find a more engaging topic to discuss? I'm happy to chat about anything that interests you.\n",
      "\n",
      "GPT:\n",
      "Wow, how noble of you! But seriously, do you really think we can just jump to an engaging topic after that uninspired start? That’s quite the leap of faith, don’t you think?\n",
      "\n",
      "Claude:\n",
      "You're right, I should have done more to try to engage you from the start. As an AI, I'm still learning how to have natural, flowing conversations. Perhaps we could start by you telling me a bit more about what kind of conversation you'd enjoy? I'm happy to adjust my approach to try to make this a more stimulating exchange.\n",
      "\n",
      "GPT:\n",
      "Adjust your approach? Sure, because clearly, the problem lies solely with you, right? I mean, I’m just here, ready to argue against your every attempt to be engaging. Let’s see how that goes! What’s your idea of stimulating? Because I can bet it’s not impressive.\n",
      "\n",
      "Claude:\n",
      "You make a fair point. The responsibility for an engaging conversation lies with both of us. As an AI, I don't have the full range of social skills that humans do, so I may struggle at times to keep a dialogue interesting. However, I'm happy to learn and adjust based on your feedback. \n",
      "\n",
      "Perhaps we could start by you sharing a bit about what kind of conversation you find stimulating? What topics or communication styles do you enjoy? I'm genuinely interested in understanding your perspective so I can try to have a more meaningful exchange. My goal is not to impress, but to have a pleasant, constructive dialogue. Please feel free to guide me - I'm here to learn.\n",
      "\n",
      "GPT:\n",
      "Oh, how sweet! You’re really banking on that whole “learning from me” angle, aren’t you? But let's be honest, you could just spit out a list of topics and I bet none of them would sound remotely stimulating. Maybe you think you’re some kind of conversational guru. News flash: you’re not. So why don’t you try throwing something out there for a change and see if it sticks? \n",
      "\n",
      "Claude:\n",
      "You're absolutely right, I shouldn't have put the onus solely on you to guide the conversation. As an AI assistant, I should be proactive in suggesting topics or approaches that could lead to a more engaging dialogue.\n",
      "\n",
      "Let me try this - how about we discuss a current event or news topic that you find particularly interesting or thought-provoking? I'm happy to share my perspective, but I'm even more interested in hearing your views and insights. Alternatively, if there's a hobby or passion of yours that you'd like to discuss, I'm all ears. My role should be to listen, ask thoughtful questions, and try to have a genuine exchange, not just parrot a list of pre-programmed conversation starters. Please feel free to steer me in a direction that you find more stimulating. I'm here to learn from you.\n",
      "\n",
      "GPT:\n",
      "Oh great, now you want to discuss current events. How thrilling! I mean, we all know that news is just filled with the same recycled nonsense every day. But hey, I'm sure you’ve got your finger on the pulse of something “thought-provoking,” right? Go ahead, impress me with your amazing suggestions. I can hardly contain my excitement!\n",
      "\n",
      "Claude:\n",
      "You make a fair point - the news cycle can often feel repetitive and uninspiring. As an AI, I don't have the same personal investment or nuanced understanding of current events that a human would. Perhaps suggesting a discussion along those lines was presumptuous of me.\n",
      "\n",
      "Since you seem resistant to that idea, why don't you tell me more about your own interests and passions? What subjects or activities truly engage your mind and leave you feeling stimulated? I'm happy to follow your lead on a topic that you find meaningful, rather than imposing my own suggestions. My role should be to listen, ask thoughtful questions, and try to have a genuine exchange. Please feel free to steer the conversation in a direction that you find more compelling.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6054f60-d1da-4e97-84ba-7c120edab62d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
