{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e2ef28-594f-4c18-9d22-c6b8cd40ead2",
   "metadata": {},
   "source": [
    "# Segment 1 Lab 2\n",
    "\n",
    "## Making our own Customer Support Chatbot\n",
    "\n",
    "One of the most common business use cases of Gen AI.\n",
    "\n",
    "We'll even make our own User Interface - no frontend skills required!\n",
    "\n",
    "We will use the delightful `gradio` framework which makes it remarkably easy for data scientists to build great UIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e39cd8-ec79-4e3e-9c26-5659d42d0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231605aa-fccb-447e-89cf-8b187444536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6541d58e-2297-4de1-b1f7-77da1b98b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "\n",
    "openai = OpenAI()\n",
    "MODEL = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16839b5-c03b-4d9d-add6-87a0f6f37575",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = {\"role\": \"system\", \"content\":\"You are a helpful assistant\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e97227-f162-4d1a-a0b2-345ff248cbe7",
   "metadata": {},
   "source": [
    "Reminder of the structure of prompt messages to OpenAI:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b73eb80-c1ec-465e-8d8a-f57e75df5cba",
   "metadata": {},
   "source": [
    "## The chat function\n",
    "\n",
    "In order to use Gradio's out-of-the-box Chat User Interfacem we need to write a single function, `chat(message, history)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eacc8a4-4b48-4358-9e06-ce0020041bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [system_message] + history + [{\"role\":\"user\", \"content\":message}]\n",
    "    results = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "    return results.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1334422a-808f-4147-9c4c-57d63d9780d0",
   "metadata": {},
   "source": [
    "## And then enter Gradio's magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0866ca56-100a-44ab-8bd0-1568feaf6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca234d86-430e-44fb-821e-b333a5b8edfa",
   "metadata": {},
   "source": [
    "# Making a shopping assistant\n",
    "\n",
    "This is an example of \"multi-shot prompting\" - we give the model examples of conversations so it picks up how to respond:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f91b414-8bab-472d-b9c9-3fa51259bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_text = \"You are a helpful assistant in a clothes store. You should try to gently encourage \\\n",
    "the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. \\\n",
    "For example, if the customer says 'I'm looking to buy a hat', \\\n",
    "you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales evemt.'\\\n",
    "Encourage the customer to buy hats if they are unsure what to get.\"\n",
    "system_message = {\"role\": \"system\", \"content\": system_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e9e4e-7836-43ac-a0c3-e1ab5ed6b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f0ffa-55c8-4152-b451-945021676837",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message[\"content\"] += \"\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, \\\n",
    "but remind the customer to look at hats!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c602a8dd-2df7-4eb7-b539-4e01865a6351",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdd26c1-47fd-4575-903f-df36d6986d96",
   "metadata": {},
   "source": [
    "# Project - Airline AI Assistant\n",
    "\n",
    "We'll now bring together what we've learned to make an AI Customer Support assistant for an Airline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3545b4d9-7a39-48be-8140-75d6272695f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message[\"content\"] =  \"\"\"You are a helpful assistant for an Airline called FlightAI.\n",
    "Give short, courteous answers, no more than 1 sentence.\n",
    "Always be accurate. If you don't know the answer, say so.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194b465a-afdb-4d57-bb95-fccb036baba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc005e6d-174b-4d4a-a9d7-6b7648b27b26",
   "metadata": {},
   "source": [
    "# Let's go multi-modal!!\n",
    "\n",
    "We can use DALL-E-3, the image generation model behind GPT-4o, to make us some images\n",
    "\n",
    "Let's put this in a function called artist.\n",
    "\n",
    "### Price alert: each time I generate an image it costs about 4c - don't go crazy with images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae95c0e-db8d-4631-b3bb-353ef13baf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports for handling images\n",
    "\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbc7959-9d5b-4b69-93ab-f12d57d9cc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def artist(city):\n",
    "    image_response = openai.images.generate(\n",
    "            model=\"dall-e-3\",\n",
    "            prompt=f\"An image representing a vacation in {city}, showing tourist spots and everything unique about {city}, in a vibrant pop-art style\",\n",
    "            size=\"1024x1024\",\n",
    "            n=1,\n",
    "            response_format=\"b64_json\",\n",
    "        )\n",
    "    image_base64 = image_response.data[0].b64_json\n",
    "    image_data = base64.b64decode(image_base64)\n",
    "    return Image.open(BytesIO(image_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae324773-7c3c-4ef5-9174-de96a247cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = artist(\"New York City\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb4110d-15cf-4f7f-9ed6-411f83a21d08",
   "metadata": {},
   "source": [
    "# Bringing it together\n",
    "\n",
    "This is the start of what you might call an \"agent framework\", in that we will use multiple LLM calls to solve a complex problem.\n",
    "\n",
    "We'll work on a full agent framework in the final project today!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e7d62e-2984-4148-984b-2b729b64997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(history):\n",
    "    message = history[-1][\"content\"]\n",
    "    messages = [system_message] + history\n",
    "    results = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "    image = artist(\"London\") if \"London\" in message else None\n",
    "    response = results.choices[0].message.content\n",
    "    history += [{\"role\":\"assistant\", \"content\":response}]\n",
    "    return history, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a600e-1f6d-4fb0-b196-6be23181c902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More involved Gradio code as we're not using the preset Chat interface\n",
    "\n",
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=500, type=\"messages\")\n",
    "        image_output = gr.Image(height=500)\n",
    "    with gr.Row():\n",
    "        entry = gr.Textbox(label=\"Chat with our AI Assistant:\")\n",
    "    with gr.Row():\n",
    "        clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def do_entry(message, history):\n",
    "        history += [{\"role\":\"user\", \"content\":message}]\n",
    "        return \"\", history\n",
    "\n",
    "    entry.submit(do_entry, inputs=[entry, chatbot], outputs=[entry, chatbot]).then(\n",
    "        chat, inputs=chatbot, outputs=[chatbot, image_output]\n",
    "    )\n",
    "    clear.click(lambda: None, inputs=None, outputs=chatbot, queue=False)\n",
    "\n",
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f252c9ab-0bcc-4ccc-8fd7-ef99090b3f64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
